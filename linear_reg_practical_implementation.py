# -*- coding: utf-8 -*-
"""Linear_reg_Practical_Implementation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Sfp2sfdSU0ihSBxJfVyizsleNkqY48LX

## Importing necessary libraries and Data
"""

import pandas as pd

from sklearn.datasets import fetch_california_housing

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

"""## Data Preprocessing"""

df = fetch_california_housing()

df

Housing_dataset= pd.DataFrame(df.data)
Housing_dataset

Housing_dataset.columns= df.feature_names

Housing_dataset.head()

#Independent features and dependent features
 x=Housing_dataset
 y=df.target

y

#Train test split
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)

x_train

x_test

#Standardization
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

"""* What is difference between normalization and standardization ?
 - Standardization transforms the data in such a way that it has a mean of 0 and a standard deviation of 1. It centers the data around the mean.
 -The formula for standardization is: z = (x - mean) / std_dev, where z is the standardized value, x is the original value, mean is the mean of the data, and std_dev is the standard deviation.
 - Standardization is less sensitive to outliers because it uses the mean and standard deviation, which are not influenced by extreme values.
 - It is commonly used when the data does not follow a specific distribution and the scale of features is important.
 - Normalization scales the data to a fixed range, typically between 0 and 1. It maintains the relative relationships between data points.
 - The formula for normalization is: x_scaled = (x - min) / (max - min), where
  - x_scaled is the normalized value, x is the original value, min is the minimum value in the data, and max is the maximum value.
 - Normalization is sensitive to outliers because it uses the minimum and maximum values, which can be influenced by extreme values.
 - It is commonly used when the distribution of the data is unknown or when the scale of features needs to be comparable

* why scaler.fit_transform and scaler.transform in x_train and x_test respectively ?
 - This could be understood as follows. Lets suppose that we have done standardization of training data then the scaler.fit_transform will make the scaler to learn the mean and std dev parameters of the training set and then calling only transform method will apply the transformation with same parameters (mean, std dev)learned to testing data. If we apply the scaler.fit_transform to testing then the method will know the mean and std dev of test set and the same it will use for train set and as model will be training on train set it will know the distribution pattern of the test data i.e. data leakage.

* What does scaler.fit_transform and scaler.transform do ? why do we need to use them ?
 - scaler.transform  apply the scaling parameters that we lean from scaler.fit_transform. i.e. fit function fits the x and y
"""

x_train=scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

x_train

x_test

# scaler.inverse_transform(x_train) #To get original values back

"""##Model Preparation"""

from sklearn.linear_model import LinearRegression
## Cross validation
from sklearn.model_selection  import cross_val_score

regression=LinearRegression()
regression.fit(x_train,y_train)

mse= cross_val_score(regression, x_train, y_train, scoring='neg_mean_squared_error', cv=5)
#The purpose of negating the MSE is to align with the convention of scikit-learn, where higher values of the scoring metric are desirable.
#By negating the MSE, scikit-learn treats it as a score that should be maximized, even though minimizing MSE is the desired goal

mean_mse=np.mean(mse)
mean_mse

"""##Prediction"""

reg_pred=regression.predict(x_test)

reg_pred

import seaborn as sns
sns.displot(reg_pred - y_test, kind='kde')

"""The error is following the gaussian distibution and hence model is predcting well"""

#In summary, the R-squared score provides an indication of how well a regression
#model fits the data and explains the variance in the target variable.
from sklearn.metrics import r2_score
score= r2_score(reg_pred, y_test)
score

